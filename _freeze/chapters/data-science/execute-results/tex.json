{
  "hash": "9cc409629bc66dd2ea6645d43545dee9",
  "result": {
    "markdown": "# Democratizing data science with open curricula and user-friendly software tools\n\n\n\n\n\n\n## Preamble\n\nIn this chapter, we contributed to the democratization of data science for\nthree key audiences: \n1) high school students who wish to learn how to code for a potential career in data science, \n2) academics who wish to learn how to code for reproducible research, and \n3) scientists who wish to apply machine learning methods toward their areas of study.\nWe developed a curriculum to teach the basics of Python for data science via a\nGirls Who Code club, a curriculum to teach introductory programming for\nreproducible research via Software Carpentry workshops, and an R package\nthat implements current best practices in machine learning for novice practitioners.\nThese free and open source contributions make data science education more \naccessible to a range of audiences and promote responsible use of data science\nmethods.\n\n<!-- GIRLS WHO CODE -->\n\n## Teaching Python for Data Science: Collaborative development of a modular and interactive curriculum\n\nThis paper was originally published in 2021 in the Journal of Open Source\nEducation with the following co-authors:\nMarlena Duda^\\*^, Kelly L. Sovacool^\\*^, Negar Farzaneh, Vy Kim Nguyen,\nSarah E. Haynes, Hayley Falk, Katherine L. Furman, Logan A. Walker,\nRucheng Diao, Morgan Oneka, Audrey C. Drotos, Alana Woloshin,\nGabrielle A. Dotson, April Kriebel, Lucy Meng, Stephanie N. Thiede,\nZena Lapp, and Brooke N. Wolford [@duda_teaching_2021].\n\n^\\*^Indicates co-first author\n\n### Summary\n\nWe are bioinformatics trainees at the University of Michigan who started a\nlocal chapter of Girls Who Code to provide a fun and supportive environment for\nhigh school women to learn the power of coding.\nOur goal was to cover basic coding topics and data science concepts through\nlive coding and hands-on practice.\nHowever, we could not find a resource that exactly met our needs.\nTherefore, over the past three years, we have developed a curriculum and\ninstructional format using Jupyter notebooks to effectively teach introductory\nPython for data science.\nThis method, inspired by The Carpentries organization, uses bite-sized lessons\nfollowed by independent practice time to reinforce coding concepts, and\nculminates in a data science capstone project using real-world data.\nWe believe our open curriculum is a valuable resource to the wider education\ncommunity and hope that educators will use and improve our lessons, practice\nproblems, and teaching best practices.\nAnyone can contribute to our Open Educational Resources [on\nGitHub](https://github.com/GWC-DCMB).\n\n### Statement of Need\n\nAs women bioinformatics trainees at the University of Michigan (U-M), we\nexperience the gender gap in our field first-hand.\nDuring the 1974-1975 academic year, women achieved 18.9% of total Bachelor's\ndegrees in computer and information sciences in the US [@nces_digest_2012].\nBy 1983-1984 this peaked at 37.1%, but fell to 17.6% by 2010-2011.\nWe also see this national trend in the training of the next generation of\nBioinformaticians at Michigan Medicine.\nSince accepting its first students in 2001, the U-M Bioinformatics Graduate\nProgram has graduated 66 male and 22 female doctorates as of 2019.\nThis disparity begins at the applicant level; during 2016-2019 the average\npercentage of females applying directly to the Bioinformatics PhD program was\n35.2%, and the average percentage of female applicants listing Bioinformatics as\nfirst or second choice in the Program in Biomedical Sciences, U-M's biomedical\nPhD umbrella program was 41%.\n\nPrevious research on women's educational experiences in science, technology,\nengineering, and mathematics (STEM) have produced various explanations for\npersistent gender disparities [@benbow_gender_2016].\nOne explanation is that women often experience stereotype threats that\nnegatively influence their math and science performance and deter them from\npursuing STEM as a career [@hill_why_2010].\nThe majority of our organization's founding graduate students (all women) began\ncoding in our undergraduate careers or later.\nWe wanted to provide a safe environment for local high school women to develop\nconfidence in themselves and their computational skills before college, and be\nexposed to successful women role models in STEM to counter negative stereotypes.\n\nGirls Who Code, a national organization whose mission\nis to close the gender gap in technology\n[@saujani_girls_2015], was founded in 2012.\nBecause of our personal experiences and the paucity of women in our field\n[@nces_digest_2012; @bonham_women_2017], we began a Girls Who Code student\norganization at the University of Michigan in 2017.\nFor the past four academic years we have registered annually as a recognized\nGirls Who Code Club because the national organization provides name recognition,\ncurriculum resources, guidance for a Capstone Impact Project, and a framework\nfor launching a coding club.\nParticipants in the Club attend weekly meetings at the University of Michigan\n(when the club is run in person rather than virtually), and are thus largely\nhigh school women from the Ann Arbor area.\nIn 2019 we launched our own summer program, the Data Science Summer Experience.\nWhen held in person, the Summer Experience is hosted in Detroit to provide the\nopportunity for high school women outside of Ann Arbor to learn coding skills in\nan inclusive environment.\n\nThe national Girls Who Code organization provides a curriculum that teaches\nwebsite and application development through programming languages like HTML and\nJava; however, our biomedical science graduate students generally have limited\nexperience with these languages and with web development.\nIn contrast, many of us have extensive experience performing data science\nusing the Python programming language.\nData Scientist was rated the #1 job in America by Glassdoor in 2016-2019, #3 in\n2020, and #2 in 2021 [@stansell_breaking_2019].\nFurthermore, Python is the most popular programming language according to the\nPYPL PopularitY of Programming Language Index [@noauthor_pypl_nodate].\nTherefore, we believe career exploration in data science using the Python\nprogramming language will optimally prepare our learners for careers that\nprovide financial stability and upward economic mobility.\nBy leveraging the data science expertise of our Club facilitators (hereafter\ntermed instructors), we created a specialized curriculum focused on\ncomputational data science in the Python programming language.\n\nGirls Who Code encourages participants to learn programming skills while working\non an Impact Project website or application throughout the Club\n[@hq_gallery_2021].\nWe created an open source Data Science curriculum that teaches the requisite\nPython and statistics skills to complete a Capstone Project, where learners\nexplore, analyze, and present a data set of their choosing.\nUsing this curriculum, we employ participatory live coding, where learners type\nand run code along with the instructor in real time.\nUsing paired activities, our curriculum follows the \"I do, we do, you do\"\ndidactic paradigm [@fisher_better_2013].\nWe provide open source resources for both in-person and virtual versions of our\ncurriculum, including videos corresponding to each lesson.\nWhile we developed this curriculum for our Girls Who Code Club and Summer\nExperience, we believe that it can be widely used for teaching introductory\ncoding for data science.\n\n### Collaborative Curriculum Development\n\nWe assembled a team of volunteers involved in our club to develop a custom\ncurriculum to teach introductory Python for data science.\nWe chose the content based on what our learners would need to learn\nto complete a small data analysis project and communicate their findings\nto their peers.\nWe divided the content by topic into Jupyter notebooks for each lesson, with\neach lesson taking approximately 15-20 minutes to teach via live coding.\nEvery lesson has a corresponding practice notebook with additional exercises on\nthe same content taught in the lesson, but using different data or variables.\nWe used a similar development workflow as the U-M Carpentries curriculum\n[@lapp_developing_2022].\nBriefly, we hosted the curriculum notebooks in a public GitHub repository to\nfacilitate collaborative development and peer review using pull requests.\nIn the initial curriculum drafting phase, developers were assigned lesson and\npractice notebooks to write.\nOnce the draft of a lesson was completed, the writer opened a pull request and\nasked for review from a different developer.\nThe reviewer then provided feedback and approved the pull request to be merged\ninto the main branch after the writer made any requested changes.\nThis way, more than one person viewed each notebook before it could be\nincorporated into the public curriculum, which reduced mistakes and ensured\nhigher quality content.\nWhile teaching from the curriculum at the first Data Science Summer Experience,\ninstructors took notes on their experience and made revisions afterward.\nMaintainers continue to monitor the repository and resolve issues as they arise.\n\nFollowing the onset of the COVID-19 pandemic, we quickly pivoted our club to a\nvirtual format.\nIn preparation for the 2020 Summer Experience, we switched to a flipped\nclassroom style following feedback from our club participants that it was too\ndifficult to follow along live coding via Zoom \n(see [Instructional Design](#instructional-design)).\n\n### Curriculum\n\nOur curriculum was designed for high school students with no prior coding\nexperience who are interested in learning Python programming for data science.\nHowever, this course material would be useful for anyone interested in\nteaching or learning basic programming for data analysis.\n\n#### Learning Objectives\n\nThe learning objectives of this curriculum are:\n\n1. Write code in Python with correct syntax and following best practices.\n2. Implement fundamental programming concepts when presented with a programmatic\n   problem set.\n3. Apply data analysis to real world data to answer scientific questions.\n4. Create informative summary statistics and data visualizations in Python.\n\nThese skills provide a solid foundation for basic data analysis in Python.\nParticipation in our program exposes learners to the many ways coding and data\nscience can be impactful across many disciplines.\n\n#### Course Content\n\nOur curriculum design consists of 27 lessons broken up into 5 modules that cover\nJupyter notebook setup, Python coding fundamentals, use of essential data\nscience packages including pandas and numpy, basic statistical analyses, and\nplotting using seaborn and matplotlib (@fig-gwc-lessons) [@harris_array_2020;\n@waskom_seaborn_2021; @hunter_matplotlib_2007].\nEach lesson consists of a lesson notebook and a practice notebook containing similar\nexercises for the learner to complete on their own following the lesson.\n\n::: {#fig-gwc-lessons fig-cap=\"Lesson modules.\nAll Jupyter notebooks are available on GitHub\n(<https://github.com/GWC-DCMB/curriculum-notebooks>).\"}\n![](/papers/girls-who-code/paper/lesson-modules.png){width=\"6.5in\" height=\"3.7in\"}\n:::\n\nEach lesson builds on those before it, beginning with relevant content reminders\nfrom the previous lessons and ending with a concise summary of the skills\npresented within.\nAs they progress through the curriculum, the learners begin simultaneously\nworking on a data science project using a real world\ndataset of their choosing.\nWhile more time is dedicated to lessons early in the program, the formal\ncurriculum tapers off until the learners are solely applying their skills to the\ndata science project.\nThrough this Capstone Project, learners gain practical experience with each\nskill as they learn it in the lessons; including importing and cleaning data,\ndata visualization, and basic statistical analyses.\n\n### Instructional Design\n\nWe modeled our instructional design in the style of Software Carpentry [@wilson_software_2016].\n\n1. Each lesson begins with a recapping of the relevant core skills presented in\n   the previous lessons.\n1. All lessons are designed to be taught via 15-minute live-coding sessions.\n   This method is used by [The Carpentries](https://carpentries.org/) and is\n   demonstrated to be an effective method that engages learners\n   [@wilson_software_2016; @nederbragt_ten_2020] since learners must actively\n   engage with the material and deal with errors and bugs as they arise.\n1. Each lesson ends with a summary of core skills presented within the material.\n1. Each short lesson is also accompanied by a subsequent 10-minute independent\n   practice, providing further opportunity for practical experience implementing\n   the coding skill at hand and testing learners' understanding of the content.\n\nTo better facilitate virtual instruction during the COVID-19 pandemic, we\nswitched to a flipped classroom.\nPrior to meeting, learners watch videos of instructors explaining the material\nthrough \"live\" coding and code along in the lesson notebook with while watching\nthe video.\nEach video shows the Jupyter notebook alongside the instructor themselves\nteaching. Learners then complete a practice notebook corresponding to the lesson.\nDuring the virtual meeting time, instructors answer questions and review the\ncore concepts in the practice exercises.\nThis virtual format is especially beneficial because it 1) allows learners to\nlearn at their own pace, and 2) enables dissemination of our curriculum to a\nwider audience interested in learning introductory Python programming for data\nscience.\n\nFor both in-person and virtual instruction, once learners have completed the\nFundamentals module and reach the Data Science Essentials module they begin\nsimultaneous work on their data science projects.\nProjects are completed in a pair programming style, where partners take turns\nassuming the \"driver\" (i.e. the typer) and \"navigator\" (i.e. the helper) roles\n[@hannay_effectiveness_2009].\nSwitching off in this way helps both partners assume equal responsibility for\nthe project workload, but more importantly it enables improved knowledge\ntransfer through peer-to-peer learning.\nThe culmination of the project is a presentation to peers, instructors, and\nfamily members.\nThrough this process learners gain hands-on experience coding, cleaning data,\nperforming statistical analyses, creating informative data visualizations, and\ncommunicating their results to others.\n\nIn addition to our coding curriculum, another key component of our programming\nis hosting women guest speakers from diverse fields across academia and\nindustry.\nOur guest speakers come to discuss the journey they have taken to their career\npaths as well as how they utilize programming and data science in their jobs.\nThese varied perspectives are extremely valuable to our learners as they provide\nseveral practical examples of programming careers in the real world, and expose\nthem to successful women in STEM.\n\n#### Experience of Use\n\nWe have used this curriculum to teach the Data Science Summer Experience and\nGirls Who Code Club in person in 2019 and virtually in 2020-2021.\nFor both in-person and virtual instances, we had several instructors present at\neach session to answer questions and help learners debug.\nFurthermore, one or two instructors were assigned to each project group to help\nlearners define data analysis questions, develop and execute a data analysis\nplan, visualize and communicate their findings, and troubleshoot coding\nproblems.\nProjects have ranged from investigating exoplanets to studying the genomics of\npsoriasis.\n\nWe credit the success of our curriculum not only to the skill of the\ninstructors, but also to the way we organized and executed the lessons and\nproject:\n\n1. The instructors and learners used [Google Colaboratory\n(Colab)](https://colab.research.google.com/) to write and execute code in\nJupyter notebooks. We chose this option because learners do not have to install\nany programs to use Google Colab and can easily open and edit the Jupyter\nnotebooks from GitHub. When meeting in person, most learners use Google\nChromebooks which have limited programming capabilities, but easy use of a web\nbrowser.\n1. Assigning instructors to groups allowed learners to build a more personal\nconnection with their instructors, making them feel more comfortable asking\nquestions.\n1. Group projects were performed using pair programming to allow learners to\ncollaborate and learn from each other.\n1. We used the \"sticky note\" system from The Carpentries by which learners can\nask for help by putting up a colored sticky note (or a Zoom emoji in the case of\nvirtual meetings) [@becker_responding_2016].\n1. We exposed the learners to different aspects of data science by bringing in\nwomen guest speakers from academics and industry. This allowed them to better\nput what they were learning into context, think about how they might use the\nskills they were learning in potential future careers, and exposed them to\nsuccessful women in STEM.\n\n##### Learner experiences\n\nWe surveyed learners anonymously after each Club and Summer Experience and found\nthat most felt that their skills in Python programming, problem solving,\ncritical thinking, and collaboration had improved (@fig-gwc-skills).\nFurthermore, on a 10 question skills assessment during the 2019-2020 instance of\nthe Club, the average increase in correct answers between the first meeting and\nthe last meeting was 4.2 with a standard deviation of 2.8 (N=5 respondents).\nWe also surveyed Club and Summer Experience alumni and found that 75% (N=20)\nwant to pursue a STEM career. 62% (N=21) are still coding.\nOn a 5-point scale from 'Strongly Disagree' to 'Strongly Agree,' the average\nanswer for 'My participation in GWC impacted my career aspirations' is 4\n(s.d.=0.9), with 4.5 (s.d.=0.6) for 'Participating in GWC made me feel more\nconfident in analyzing data' and 3.9 (s.d.=1) for 'Participating in GWC made me\nmore confident in myself.'\n\n::: {#fig-gwc-skills fig-cap=\"Post-survey responses.\nLearners were asked if they felt that their\nskills in Python programming, problem solving, critical thinking, and\ncollaboration had improved.\"}\n![](/papers/girls-who-code/paper/skill_plots.png){width=\"6.5in\" height=\"4.3in\"}\n:::\n\nOverwhelmingly, learners' favorite parts of the program are the guest speakers\nand the project.\nThese aspects of our curriculum expose them to new fields and allow them to\napply their newfound coding skills to asking an interesting question.\nA 2021 Club learner shared, \"I plan to go to college for Computer Science and\nget a robotics minor when my college offers it.\nGWC has inspired me to consider pursuing a Masters or PhD in CS as well as take\nsome electives in Data Science.\"\nFive of our 86 alumni have gone on to perform research with U-M faculty members,\nwith one presenting her work at an international conference.\nIn fact, about a third of participants claim that they are now more interested\nin pursuing a career in computer or data science compared to before their Girls\nWho Code experience.\n\n### Acknowledgements\n\nWe would like to acknowledge our faculty co-sponsors Maureen Sartor &\nCristina Mitrea.\nWe appreciate the continued support of U-M DCMB staff and faculty including\nJulia Eussen, Mary Freer, Linda Peasley, Jane Wiesner, Brian Athey, and\nMargit Burmeister.\nWe are grateful for the resources provided by the national Girls Who Code\norganization.\n\nOur programming is made possible by the dedication of past and present Executive\nCommittee members, Club and Summer Experience Facilitators, and Capstone Project\nmentors including\nShweta Ramdas, Alex Weber, Arushi Varshney, Sophie Hoffman, Hojae Lee, Ruma\nDeb, Saige Rutherford, Michelle McNulty, Bailey Peck, Chloe Whicker, Carolina\nRojas Ramirez, Verity Sturm, Zoe Drasner, Sarah Latto, Emily Roberts, Angel Chu,\nVivek Rai, Hillary Miller, Ashton Baker, Murchtricia Jones, Lauren Jepsen,\nAubrey Annis, Awanti Sambarey, Mengtong Hu, Maribel Okiye, Yingxiao Zhang, and\nNeslihan Bisgin.\n\nWe are grateful for the funding, assistance, and other support provided to our\nstudent organization from the following sponsors:\nthe U-M Department of Computational Medicine and Bioinformatics,\nthe U-M Department of Biostatistics,\nthe U-M Department of Statistics,\nthe U-M Office of Graduate and Postdoctoral Studies,\nthe U-M Endowment in Basic Sciences,\nthe U-M Detroit Center,\nthe U-M Life Sciences Institute,\nthe U-M Office of Research,\nthe Michigan Council of Women in Technology Foundation,\nDELL Technologies,\nCisco Systems,\nZingerman's Delicatessen,\nthe Girls Who Code Support Fund,\nand anonymous donations from Giving Blue Day 2019.\n\nWe also thank the learners who have participated in our Club and Summer\nExperience events.\n\n### Funding\n\nMD, ACD, ZL, and BNW received support from the National Science Foundation\nGraduate Research Fellowship Program under Grant No. DGE 1256260.\nAny opinions, findings, and conclusions or recommendations expressed in this\nmaterial are those of the authors and do not necessarily reflect the views of\nthe National Science Foundation.\n\nMD, KLS, NF, and VKN received support from the NIH Training Program in\nBioinformatics (T32 GM070449).\nNF was supported by the National Institute of Health (NIH) Ruth L. Kirschstein\nNational Research Service Award (NRSA) Individual Predoctoral Fellowship\nProgram (F31 LM012946-01).\nVKN was supported by a NIH Research Project Grant on Breast Cancer Disparities\n(RO1-ES028802) and the CDC through the National Institute for Occupational\nSafety and Health (NIOSH) Pilot Project Research Training Program\n(T42-OH008455).\nKLF received support from The University of Michigan NIDA Training Program in\nNeuroscience (T32-DA7281) and from the NIH Early Stage Training in the\nNeurosciences Training Grant (T32-NS076401).\nMO received support from the Advanced Proteome Informatics of Cancer Training\nGrant (T32 CA140044).\nSNT was supported by the Molecular Mechanisms in Microbial Pathogenesis training\ngrant (NIH T32 AI007528).\nZL and BNW received support from the NIH Training Program in Genomic Science\n(T32-HG000040-22).\n\n### Author Contributions\n\nMD, KLS, ZL, and BNW wrote the initial draft of the manuscript.\nAll authors contributed to the curriculum and reviewed the manuscript.\n\n### Conflicts of Interest\n\nNone.\n\n<!-- SOFTWARE CARPENTRY -->\n\n## Developing and deploying an integrated workshop curriculum teaching computational skills for reproducible research\n\nThis paper was original published in 2022 in the Journal of Open Source\nEducation with the following co-authors:\nZena Lapp^\\*^, Kelly L. Sovacool^\\*^, Nick Lesniak, Dana King,\nCatherine Barnier, Matthew Flickinger, Jule Krüger, Courtney R. Armour,\nMaya M. Lapp, Jason Tallant, Rucheng Diao, Morgan Oneka, Sarah Tomkovich,\nJacqueline Moltzau Anderson, Sarah K. Lucas, and Patrick D. Schloss\n[@lapp_developing_2022].\n\n^\\*^Indicates co-first author\n\n### Summary\n\nInspired by well-established material and pedagogy provided by The Carpentries\n[@wilson_software_2016], we developed a two-day workshop curriculum that teaches\nintroductory R programming for managing, analyzing, plotting and reporting data\nusing packages from the tidyverse [@wickham_welcome_2019], the Unix shell,\nversion control with git, and GitHub.\nWhile the official Software Carpentry curriculum is comprehensive, we found\nthat it contains too much content for  a two-day workshop.\nWe also felt that the independent nature of the lessons left learners confused\nabout how to integrate the newly acquired programming skills in their own work.\nThus, we developed\n[a new curriculum](https://umcarpentries.org/intro-curriculum-r/)\nthat aims to teach novices how to implement reproducible research principles in\ntheir own data analysis.\nThe curriculum integrates live coding lessons with individual-level and\ngroup-based practice exercises, and also serves as a succinct resource\nthat learners can reference both during and after the workshop.\nMoreover, it lowers the entry barrier for new instructors as they do not\nhave to develop their own teaching materials or sift through extensive content.\nWe developed this curriculum during a two-day sprint, successfully used it to\nhost a two-day virtual workshop with almost 40 participants, and updated the\nmaterial based on instructor and learner feedback.\nWe hope that our new curriculum will prove useful to future instructors\ninterested in teaching workshops with similar learning objectives.\n\n### Statement of Need\n\nFor the past five years, the University of Michigan instance of The Carpentries has taught workshops\nusing versions of curriculum originally created by The Carpentries organization.\nIn that time, our instructors found several advantages and disadvantages to\nusing the original Software Carpentry curriculum.\nSome of the advantages were that any programming language lesson (e.g., R or\nPython) could be paired with lessons on the Unix shell and version control,\nlessons had been refined by many contributors over the years and taught at\nworkshops around the world, and the instructional design demonstrated good\npedagogy for teaching novice data science practitioners.\nHowever, The Carpentries materials have evolved from lesson plans to reference materials, and thus there was too much content for the time\navailable during a two-day workshop.\nAs a result, workshops taught with this material were inconsistent  depending on\nwho was teaching, and new instructors faced an overwhelming amount of work to\nprepare for their first workshop.\nFurthermore, the modular nature of the curriculum meant that each lesson was\nindependent from the others, so it was not apparent to learners how all of the\nskills could be integrated for the purpose of a reproducible research project.\n\nGiven these constraints, we sought to create a new curriculum that would allow\nus to teach computational skills in an integrated manner, demonstrate the\nreproducible research workflows we use in our own work, deliver an appropriate\nand consistent amount of content, and reduce the burden for new instructors to\nget involved, all while maintaining the same inclusive pedagogy that has been\nrefined by The Carpentries organization.\n\n### Collaborative Curriculum Development\n\nWe drew on the expertise of The Carpentries community at the\nUniversity of Michigan to develop a custom curriculum that would meet our goals\n(@fig-swc-framework).\nTo start, we organized a two-day sprint, where members of our community worked\ncollaboratively to create an initial draft of the content.\nDuring the sprint, we met virtually to discuss our goals, then broke up into\nteams to work on individual lessons before coming back together to review our\nprogress.\nWe hosted the curriculum in a public GitHub repository\n(https://github.com/umcarpentries/intro-curriculum-r) to facilitate\ncollaborative work and peer review using issues, branches, and pull requests.\nUnder this model, a team member created or edited content in a new branch to\nresolve an issue,\nthen created a pull request and asked for review from another team member,\nwho finally merged the changes into the default branch.\nGitHub pages automatically uses the default branch to build a website that allows us to host the polished curriculum\n(https://umcarpentries.org/intro-curriculum-r/).\nOur collaborative model ensured that at least two pairs of eyes viewed any\nchanges before they could be included in the curriculum.\nThis strategy helped us reduce mistakes and create better quality content.\n\n::: {#fig-swc-framework fig-cap=\"Curriculum development framework\"}\n![](/papers/software-carpentry/paper/development-framework.png){width=\"6.5in\" height=\"2.5in\"}\n:::\n\nFollowing the sprint, contributors finalized edits and continued to review each\nothers' pull requests to complete the alpha version of our curriculum.\nNext, we hosted a workshop for instructors to pilot the curriculum.\nWe collected feedback from the learners and instructors at the end of the pilot\nworkshop and then held a smaller half-day sprint to revise the curriculum based\non the feedback.\nCurrently, our community members are continuously able to create issues, make\nedits, and review pull requests to keep refining the curriculum for future use.\nWe are planning more workshops with new instructors who were not involved in the\noriginal curriculum development to gather their feedback.\n\n### Curriculum\n\nOur curriculum is tailored to people with no prior coding experience who want to\nlearn how to use R programming for data analysis, visualization and the\nreporting of results (@fig-swc-curriculum).\nNot only do we aim to teach our learners the basics of performing empirical data\nanalysis, we also seek to provide a rigorous framework for adhering to\nreproducible research principles that enable researchers to easily share their\nempirical work with others.\n\n::: {#fig-swc-curriculum fig-cap=\"Curriculum overview\"}\n![](/papers/software-carpentry/paper/curriculum-overview.png){width=\"6.5in\" height=\"2.1in\"}\n:::\n\n\n#### Learning Objectives\n\nThe key learning objectives for our curriculum are:\n\n1. Create clear and informative data visualizations in R, starting with messy data.\n1. Perform version control using the Unix shell and git.\n1. Create reproducible reports using R Markdown.\n1. Share code with others on GitHub.\n\nWe believe these skills provide learners with a solid foundation from which they\ncan teach themselves any additional coding skills for future use.\n\n#### Course Content\n\nOur curriculum consists of nine modules that cover software setup, data analysis\nand visualization in R, version control, sharing code, and writing reproducible\nreports (see below for more details).\nThe R programming lessons take a \"tidyverse first\" approach\n[@robinson_teach_2017] to effectively and efficiently teach learners powerful\ntools for plotting and data analysis.\nWe also set an overall goal for the workshop to make the content substantively\ninteresting and relatable to a wide audience regardless of their original\nacademic discipline or professional practice.\nSpecifically, we task our learners with producing a fictitious report to the\nUnited Nations that examines the relationship between gross domestic product\n(GDP), life expectancy, and CO~2~ emissions.\nThe nine curriculum modules are:\n\n0. Setup\n1. Welcome\n1. R for plotting (uses the tidyverse R packages [@wickham_welcome_2019])\n1. The Unix shell\n1. Git and GitHub\n1. R for data analysis (uses the tidyverse R packages [@wickham_welcome_2019])\n1. Writing reports in R Markdown (uses the rmarkdown R package [@xie_r_2018])\n1. Group practice exercises\n1. Where to go from here\n\nEach lesson builds on the previous ones.\nThe Unix shell, git, and GitHub are introduced using the files generated in the\nR for plotting lesson.\nThe lesson content for subsequent modules is then intermittently committed and\npushed to GitHub.\nThe 'Writing reports in R Markdown' lesson combines all of the skills learned\npreviously to produce a report that one could share with the United Nations.\nNext, learners put everything they have learned into practice by forming small\ngroups and working on practice problems that cover the entire course content\n ([\"Integrating it all together: Paired exercise\"](https://umcarpentries.org/intro-curriculum-r/05-r-markdown/index.html#integrating-it-all-together-paired-exercise)).\nThe workshop completes with a short module recapping everything that the\ncurriculum covered as well as offering suggestions on how learners can continue\nto get help and keep learning once the workshop ends.\n\n#### Instructional Design\n<!-- teaching philosophy / pedagogy -->\n\nOur modules and teaching suggestions are developed in the style of [Software\nCarpentry](https://software-carpentry.org/):\n\n1. Each module contains learning objectives at the beginning of each lesson and\na summary of key points at the end.\n1. The five core modules (2 to 6) are designed to be taught via live coding of\nthe content to learners.\nThis is a central feature of Carpentries lessons, and we believe it is a great\nway to learn how to program. It requires learners to follow along and\nencounter errors that they must debug along the way, fostering additional\nquestions about the course content. It also leads to instructors making mistakes\nand then demonstrating how to deal with them in an ad hoc and iterative manner.\n1. We incorporate formative assessments in the form of short practice exercises\nthroughout each lesson such that learners can practice what they have learned, while\ninstructors can gauge learner understanding of the material.\n1. We use the \"sticky note\" system for formative assessment, where learners\nindicate their progress on exercises and request help by using different\ncolored sticky notes [@becker_responding_2016; @the_carpentries_live_2018].\nAt virtual workshops, we use Zoom reaction icons as virtual sticky\nnotes, with the red X reaction to ask for help and the green checkmark to\nindicate that an exercise was successfully completed.\n1. We have several helpers attend each workshop to address learner questions and technical issues.\n\nWe also incorporated a few additional key components into the curriculum:\n\n1. Each lesson built off of previous lessons, with the goal of creating a final report\nthat can be shared with others.\n1. We structured the curriculum such that it could be taught through an\nin-person or virtual workshop. Virtual workshops are sometimes necessary, as during the COVID-19 pandemic, but are also useful to allow people from a variety of geographic locations to instruct and attend.\n1. We not only required learners to install all software before the workshop (as The\nCarpentries also requires), but also asked them to run an example script that tests\nwhether everything is installed correctly.\nTo attend the workshop, learners were required to send screenshots of the script output to the workshop lead in\nadvance. We withheld the login details for the workshop until we received the screenshot. This ensured that any installation issues could be\naddressed before the workshop began.\n1. An extensive small group practice module towards the end of the workshop\nallowed learners to more independently practice the skills they have learned.\n1. The workshop concluded with a recap of what was covered and resources\navailable for learners to continue learning and getting help as their skills\ndevelop.\n\n#### Pilot Workshop\n<!-- experience of use -->\n\nWe piloted our curriculum during a virtual two-day Software Carpentry workshop.\nIn line with The Carpentries recommendations\n[@the_carpentries_carpentries_2018], we had four instructors and six helpers at\nthe workshop to assist with learner questions and technical issues.\nWe had thirty-nine learners of various skill levels from several different\ncountries, all of whom provided very positive reviews of the workshop.\nTo assess the effectiveness of the workshop, learners were asked to complete a\npre- and post-workshop survey administered by the Carpentries.\nBy the end of the workshop, learners on average felt more confident writing\nprograms, using programming to work with data, overcoming problems while\nprogramming, and searching for answers to technical questions online (n = 14\nsurvey respondents; see @fig-swc-survey).\nAll attendees who filled out the post-workshop survey (n = 19) would recommend\nthe workshop to others.\n\n::: {#fig-swc-survey fig-cap=\"Pre- and post-workshop survey results\"}\n![](/papers/software-carpentry/paper/survey-results.png){width=\"6.5in\" height=\"3.3in\"}\n:::\n\n##### Virtual Workshop Reflection\n\nWe credit the success of our first virtual workshop in large part due to the\ncurriculum structure and content, as well as the instructors and helpers\ninvolved.\nHowever, we also believe that the following helped make the workshop as\nsmooth as possible:\n\n1. We suggested that learners have Zoom and RStudio (or the Unix shell) open side-by-side on their computer to minimize toggling between different windows [@chen_online_2020].\n1. We used Slack for communication among instructors and helpers, as well as between helpers and learners.\nLearners asked questions in a group Slack channel where helpers could respond.\nThis allowed us to address the vast majority of learner questions and bugs quickly, clearly, and efficiently without disrupting the lesson or moving the learner to a Zoom breakout room.\nFurthermore, Slack worked much better than the Zoom chat as questions could be answered in threads, were preserved and visible to all learners regardless of whether they were connected to Zoom at the time, and didn't get lost as easily.  \n1. Whenever a learner needed more help than was possible on Slack, a helper and the\nlearner entered a Zoom breakout room together to troubleshoot.\nHowever, we tried to minimize this option as much as possible to prevent the learner from missing content\ncovered in the main room.\n\n### Acknowledgements\n\nWe thank The Carpentries organization for providing instructor training,\nworkshop protocols, and the open-source Software Carpentry curriculum upon\nwhich this curriculum is based. We also thank them for allowing us to use the\npre- and post-workshop survey results in this manuscript.\nThe Carpentries is a fiscally sponsored project of Community Initiatives, a\nregistered 501(c)3 non-profit organisation based in California, USA.\n\nWe are grateful to Victoria Alden and Scott Martin for assisting us in\norganizing and advertising our pilot workshop.\nWe thank Shelly Johnson for volunteering as a helper at the workshop and\ncontributing to the setup instructions.\nWe also thank Bennet Fauber for contributing to the setup instructions.\n\nWe thank the learners who participated in the workshop, provided feedback, and\ncompleted the surveys.\n\n### Funding\n\nSalary support for PDS came from NIH grants R01CA215574 and U01AI124255.\nKLS received support from the NIH Training Program in Bioinformatics (T32\nGM070449).\nZL received support from the National Science Foundation Graduate Research\nFellowship Program under Grant No. DGE 1256260.\nAny opinions, findings, and conclusions or recommendations expressed in this\nmaterial are those of the authors and do not necessarily reflect the views of\nthe National Science Foundation.\n\n### Author Contributions\n\nZL and KLS contributed equally. ZL is first among the co-first authors because\nKLS threatened to reject all pull requests where ZL put KLS first. :)\n\nPDS supervised the project.\nZL and KLS organized the initial sprint, led the development of the curriculum,\nand drafted the manuscript.\nZL, KLS, JK, and MML instructed at the first pilot workshop\nwhile CRA, JMA, ST, SKL, and CB assisted learners.\nAll authors contributed to the development of the curriculum.\n\n### Conflicts of Interest\n\nNone.\n\n<!-- MIKROPML -->\n\n## mikropml: User-Friendly R Package for Supervised Machine Learning Pipelines\n\nThis paper was original published in 2021 in the Journal of Open Source Software\nwith the following co-authors:\nBegüm D. Topçuoğlu^\\*^, Zena Lapp^\\*^, Kelly L. Sovacool^\\*^, Evan Snitkin,\nJenna Wiens, and Patrick D. Schloss [@topcuoglu_mikropml_2021].\n\n^\\*^Indicates co-first author\n\n### Summary\n\nMachine learning (ML) for classification and prediction based on a set of\nfeatures is used to make decisions in healthcare, economics, criminal justice\nand more. However, implementing an ML pipeline including preprocessing, model\nselection, and evaluation can be time-consuming, confusing, and difficult. Here,\nwe present [`mikropml`](http://www.schlosslab.org/mikropml/) (pronounced\n\"meek-ROPE em el\"), an easy-to-use R package that implements ML pipelines using\nregression, support vector machines, decision trees, random forest, or\ngradient-boosted trees. The package is available on\n[GitHub](https://github.com/SchlossLab/mikropml/),\n[CRAN](https://cran.r-project.org/package=mikropml), and\n[conda](https://anaconda.org/conda-forge/r-mikropml).\n\n### Statement of need\n\nMost applications of machine learning (ML) require reproducible steps for data\npre-processing, cross-validation, testing, model evaluation, and often\ninterpretation of why the model makes particular predictions. Performing these\nsteps is important, as failure to implement them can result in incorrect and\nmisleading results [@teschendorff_avoiding_2019; @wiens_no_2019].\n\nSupervised ML is widely used to recognize patterns in large datasets and to make\npredictions about outcomes of interest. Several packages including `caret`\n[@kuhn_building_2008] and `tidymodels` [@kuhn_tidymodels_2020] in R,\n`scikitlearn` [@pedregosa_scikit-learn_2011] in Python, and the H2O `autoML`\nplatform [@h2oai_h2o_2020] allow scientists to train ML models with a variety of\nalgorithms. While these packages provide the tools necessary for each ML step,\nthey do not implement a complete ML pipeline according to good practices in the\nliterature. This makes it difficult for practitioners new to ML to easily begin\nto perform ML analyses.\n\nTo enable a broader range of researchers to apply ML to their problem domains,\nwe created [`mikropml`](https://github.com/SchlossLab/mikropml/), an easy-to-use\nR package [@r_core_team_r_2023] that implements the ML pipeline created by\nTopçuoğlu _et al._ [@topcuoglu_framework_2020] in a single function that returns a trained model,\nmodel performance metrics and feature importance. `mikropml` leverages\nthe `caret` package to support several ML algorithms: linear regression,\nlogistic regression, support vector machines with a radial basis kernel,\ndecision trees, random forest, and gradient boosted trees. It incorporates good\npractices in ML training, testing, and model evaluation\n[@topcuoglu_framework_2020;@teschendorff_avoiding_2019]. Furthermore, it\nprovides data preprocessing steps based on the FIDDLE (FlexIble Data-Driven\npipeLinE) framework outlined in Tang _et al._ [@tang_democratizing_2020] and\npost-training permutation importance steps to estimate the importance of each\nfeature in the models trained [@breiman_random_2001; @fisher_all_2018].\n\n`mikropml` can be used as a starting point in the application of ML to datasets\nfrom many different fields. It has already been applied to microbiome data to\ncategorize patients with colorectal cancer [@topcuoglu_framework_2020], to\nidentify differences in genomic and clinical features associated with bacterial\ninfections [@lapp_machine_2020], and to predict gender-based biases in academic\npublishing [@hagan_women_2020].\n\n### mikropml package\n\nThe `mikropml` package includes functionality to preprocess the data, train ML\nmodels, evaluate model performance, and quantify feature importance (@fig-mikropml-pipeline).\nWe also provide\n[vignettes](http://www.schlosslab.org/mikropml/articles/index.html) and an\n[example Snakemake\nworkflow](https://github.com/SchlossLab/mikropml-snakemake-workflow)\n[@koster_snakemake_2012] to showcase how to run an ideal ML pipeline\nwith multiple different train/test data splits. The results can be visualized\nusing helper functions that use `ggplot2` [@wickham_ggplot2_2016].\n\nWhile mikropml allows users to get started quickly and facilitates\nreproducibility, it is not a replacement for understanding the ML workflow which\nis still necessary when interpreting results [@pollard_turning_2019]. To\nfacilitate understanding and enable one to tailor the code to their application,\nwe have heavily commented the code and have provided supporting documentation\nwhich can be read [online](http://www.schlosslab.org/mikropml/).\n\n#### Preprocessing data\n\nWe provide the function `preprocess_data()` to preprocess features using several\ndifferent functions from the `caret` package. `preprocess_data()` takes\ncontinuous and categorical data, re-factors categorical data into binary\nfeatures, and provides options to normalize continuous data, remove features\nwith near-zero variance, and keep only one instance of perfectly correlated\nfeatures. We set the default options based on those implemented in FIDDLE\n[@tang_democratizing_2020]. More details on how to use `preprocess_data()` can\nbe found in the accompanying\n[vignette](http://www.schlosslab.org/mikropml/articles/preprocess.html).\n\n#### Running ML\n\nThe main function in mikropml, `run_ml()`, minimally takes in the model choice\nand a data frame with an outcome column and feature columns. For model choice,\n`mikropml` currently supports logistic and linear regression [`glmnet`:\n@friedman_regularization_2010], support vector machines with a radial basis\nkernel [`kernlab`: @karatzoglou_kernlab_2004], decision trees [`rpart`:\n@therneau_rpart_2019], random forest [`randomForest`: @liaw_classication_2002],\nand gradient-boosted trees [`xgboost`:  @chen_xgboost_2020]. `run_ml()` randomly\nsplits the data into train and test sets while maintaining the distribution of\nthe outcomes found in the full dataset. It also provides the option to split the\ndata into train and test sets based on categorical variables (e.g. batch,\ngeographic location, etc.). `mikropml` uses the `caret` package\n[@kuhn_building_2008] to train and evaluate the models, and optionally\nquantifies feature importance. The output includes the best model built based on\ntuning hyperparameters in an internal and repeated cross-validation step, model\nevaluation metrics, and optional feature importances. Feature importances are\ncalculated using a permutation test, which breaks the relationship between the\nfeature and the true outcome in the test data, and measures the change in model\nperformance. This provides an intuitive metric of how individual features\ninfluence model performance and is comparable across model types, which is\nparticularly useful for model interpretation [@topcuoglu_framework_2020]. Our\n[introductory\nvignette](http://www.schlosslab.org/mikropml/articles/introduction.html)\ncontains a comprehensive tutorial on how to use `run_ml()`.\n\n::: {#fig-mikropml-pipeline fig-cap=\"The mikropml pipeline\"}\n![](/papers/mikropml/vignettes/mikRopML-pipeline.png){width=\"6.5in\" height=\"3.7in\"}\n:::\n\n#### Ideal workflow for running mikropml with many different train/test splits\n\nTo investigate the variation in model performance depending on the train and\ntest set used [@topcuoglu_framework_2020; @lapp_machine_2020], we provide\nexamples of how to `run_ml()` many times with different train/test splits and\nhow to get summary information about model performance on [a local\ncomputer](http://www.schlosslab.org/mikropml/articles/parallel.html) or on a\nhigh-performance computing cluster using a [Snakemake\nworkflow](https://github.com/SchlossLab/mikropml-snakemake-workflow).\n\n#### Tuning & visualization\n\nOne particularly important aspect of ML is hyperparameter tuning. We provide a\nreasonable range of default hyperparameters for each model type. However\npractitioners should explore whether that range is appropriate for their data,\nor if they should customize the hyperparameter range. Therefore, we provide a\nfunction `plot_hp_performance()` to plot the cross-validation performance metric\nof a single model or models built using different train/test splits. This helps\nevaluate if the hyperparameter range is being searched exhaustively and allows\nthe user to pick the ideal set. We also provide summary plots of test\nperformance metrics for the many train/test splits with different models using\n`plot_model_performance()`. Examples are described in the accompanying [vignette\non hyperparameter\ntuning](http://www.schlosslab.org/mikropml/articles/tuning.html).\n\n#### Dependencies\n\nmikropml is written in R [@r_core_team_r_2023] and depends on several packages:\n`dplyr` [@wickham_dplyr_2020], `rlang` [@henry_rlang_2020] and `caret`\n[@kuhn_building_2008]. The ML algorithms supported by `mikropml` require:\n`glmnet` [@friedman_regularization_2010], `e1071` [@meyer_e1071_2020], and\n`MLmetrics` [@yan_mlmetrics_2016] for logistic regression, `rpart2`\n[@therneau_rpart_2019] for decision trees, `randomForest`\n[@liaw_classication_2002] for random forest, `xgboost` [@chen_xgboost_2020] for\nxgboost, and `kernlab` [@karatzoglou_kernlab_2004] for support vector machines.\nWe also allow for parallelization of cross-validation and other steps using the\n`foreach`, `doFuture`, `future.apply`, and `future` packages\n[@bengtsson_futureapply_2020]. Finally, we use `ggplot2` for plotting\n[@wickham_ggplot2_2016].\n\n### Acknowledgments\n\nWe thank members of the Schloss Lab who participated in code clubs related to\nthe initial development of the pipeline, made documentation improvements, and\nprovided general feedback.\nWe also thank Nick Lesniak for designing the mikropml logo.\n\nWe thank the US Research Software Sustainability Institute (NSF #1743188) for\nproviding training to KLS at the Winter School in Research Software Engineering.\n\n### Funding\n\nSalary support for PDS came from NIH grant 1R01CA215574. KLS received support\nfrom the NIH Training Program in Bioinformatics (T32 GM070449). ZL received\nsupport from the National Science Foundation Graduate Research Fellowship\nProgram under Grant No. DGE 1256260. Any opinions, findings, and conclusions or\nrecommendations expressed in this material are those of the authors and do not\nnecessarily reflect the views of the National Science Foundation.\n\n### Author contributions\n\nBDT, ZL, and KLS contributed equally. Author order among the co-first authors\nwas determined by time since joining the project.\n\nBDT, ZL, and KLS conceptualized the study and wrote the code. KLS structured the\ncode in R package form. BDT, ZL, JW, and PDS developed methodology. PDS, ES, and\nJW supervised the project. BDT, ZL, and KLS wrote the original draft. All\nauthors reviewed and edited the manuscript.\n\n### Conflicts of interest\n\nNone.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}